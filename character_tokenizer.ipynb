{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "character_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+v4UgPMMNg9sYVMecmBJ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lhy0718/bert-character-mlm/blob/master/character_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz0IZk-q-Y1q"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELTAXKPJ1a38"
      },
      "source": [
        "# make vocab file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCsLxkvdkiwB"
      },
      "source": [
        "VOCAB_FILE = 'char-bert-base-uncased-vocab.txt'\n",
        "\n",
        "vocabs = ['[PAD]']\n",
        "\n",
        "for i in range(99):\n",
        "  vocabs.append(f'[unused{i}]')\n",
        "vocabs += ['[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
        "\n",
        "ascii = [chr(c) for c in range(ord('!'), ord('A'))]\n",
        "ascii += [chr(c) for c in range(ord('['), ord('~') + 1)]\n",
        "pieced_lower_case = ['##' + chr(c) for c in range(ord('a'), ord('z') + 1)]\n",
        "\n",
        "with open(VOCAB_FILE, 'w') as f:\n",
        "  f.write('\\n'.join(vocabs + ascii + pieced_lower_case))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G11UOW8r1v5x"
      },
      "source": [
        "# save tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdst6_-iJvj7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d2febf2-9688-4509-fd18-094012ce19cf"
      },
      "source": [
        "from transformers import BertTokenizerFast, BertForMaskedLM, BertConfig\n",
        "\n",
        "MODEL_NAME = 'char-bert-base-uncased'\n",
        "\n",
        "tokenizer = BertTokenizerFast(\n",
        "  vocab_file=VOCAB_FILE,\n",
        "  do_lower_case=False\n",
        ")\n",
        "print('vocab_size:', len(tokenizer))\n",
        "\n",
        "config = BertConfig(vocab_size=len(tokenizer))\n",
        "model = BertForMaskedLM(config)\n",
        "tokenizer.save_pretrained(MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 198\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('char-bert-base-uncased/tokenizer_config.json',\n",
              " 'char-bert-base-uncased/special_tokens_map.json',\n",
              " 'char-bert-base-uncased/vocab.txt',\n",
              " 'char-bert-base-uncased/added_tokens.json',\n",
              " 'char-bert-base-uncased/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxECKzuv1W7E"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGowDd5HOTV3",
        "outputId": "77e08daf-bee4-466b-ec7e-994d143c7adb"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "input_ids = tokenizer.encode('hello there!')\n",
        "print(input_ids)\n",
        "print([tokenizer.decode(id) for id in input_ids])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 149, 176, 183, 183, 186, 161, 179, 176, 189, 176, 104, 102]\n",
            "['[CLS]', 'h', '##e', '##l', '##l', '##o', 't', '##h', '##e', '##r', '##e', '!', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgdSEjXJ-evu",
        "outputId": "edc06a97-1db0-4f79-9dbd-b959091e8131"
      },
      "source": [
        "!zip -r char-bert-base-uncased.zip char-bert-base-uncased"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: char-bert-base-uncased/ (stored 0%)\n",
            "  adding: char-bert-base-uncased/tokenizer_config.json (deflated 42%)\n",
            "  adding: char-bert-base-uncased/vocab.txt (deflated 71%)\n",
            "  adding: char-bert-base-uncased/special_tokens_map.json (deflated 40%)\n",
            "  adding: char-bert-base-uncased/tokenizer.json (deflated 64%)\n"
          ]
        }
      ]
    }
  ]
}