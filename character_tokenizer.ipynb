{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "character_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5rbxfFs552dho6JlvOx61"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz0IZk-q-Y1q"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELTAXKPJ1a38"
      },
      "source": [
        "# make vocab file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCsLxkvdkiwB"
      },
      "source": [
        "VOCAB_FILE = 'char-bert-base-cased-vocab.txt'\n",
        "\n",
        "vocabs = ['[PAD]']\n",
        "\n",
        "for i in range(100):\n",
        "  vocabs.append(f'[unused{i}]')\n",
        "vocabs += ['[UNK]', '[CLS]', '[SEP]', '[MASK]', '[unused100]', '[unused101]']\n",
        "\n",
        "ascii = [chr(c) for c in range(33, 127)]\n",
        "lower_case = ['##' + chr(c) for c in range(ord('a'), ord('z') + 1)]\n",
        "upper_case = ['##' + chr(c) for c in range(ord('A'), ord('Z') + 1)]\n",
        "\n",
        "with open(VOCAB_FILE, 'w') as f:\n",
        "  f.write('\\n'.join(vocabs + ascii + lower_case + upper_case))"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G11UOW8r1v5x"
      },
      "source": [
        "# save tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdst6_-iJvj7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a00d2aa-d328-4df8-fc6b-9e466942b886"
      },
      "source": [
        "from transformers import BertTokenizerFast, BertForMaskedLM, BertConfig\n",
        "\n",
        "MODEL_NAME = 'char-bert-base-cased'\n",
        "\n",
        "tokenizer = BertTokenizerFast(\n",
        "  vocab_file=VOCAB_FILE,\n",
        "  do_lower_case=False\n",
        ")\n",
        "print('vocab_size:', len(tokenizer))\n",
        "\n",
        "config = BertConfig(vocab_size=len(tokenizer))\n",
        "model = BertForMaskedLM(config)\n",
        "tokenizer.save_pretrained(MODEL_NAME)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 253\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('char-bert-base-cased/tokenizer_config.json',\n",
              " 'char-bert-base-cased/special_tokens_map.json',\n",
              " 'char-bert-base-cased/vocab.txt',\n",
              " 'char-bert-base-cased/added_tokens.json',\n",
              " 'char-bert-base-cased/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxECKzuv1W7E"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGowDd5HOTV3",
        "outputId": "4531413c-d155-45d2-9f44-03d37f070ef7"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "input_ids = tokenizer.encode('hello there!')\n",
        "print(input_ids)\n",
        "print([tokenizer.decode(id) for id in input_ids])"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[102, 178, 205, 212, 212, 215, 190, 208, 205, 218, 205, 107, 103]\n",
            "['[CLS]', 'h', '##e', '##l', '##l', '##o', 't', '##h', '##e', '##r', '##e', '!', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhjntX_icwYR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}